{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import constants as cons\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', palette='deep') \n",
    "\n",
    "import matplotlib.style as style\n",
    "style.use('fivethirtyeight')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cons.file_path, index_col=0)\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### EXPLORE ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### FEATURE ENGINEERING ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's convert few columns which are categorical \n",
    "# into integer values, so that it is easy for machine to handle values.\n",
    "\n",
    "def sale(string):\n",
    "    if ('-' in string):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return int(string)\n",
    "    \n",
    "def year_built(string):\n",
    "    if string==0:\n",
    "        return np.NaN\n",
    "    elif ('-' in string):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return int(string)\n",
    "    \n",
    "df['SALE PRICE'] = df['SALE PRICE'].map(lambda x: sale(x))\n",
    "df['YEAR BUILT'] = df['YEAR BUILT'].map(lambda x: int(x))\n",
    "df['YEAR SOLD'] = df['SALE DATE'].map(lambda x: x[0:4])\n",
    "df['YEAR SOLD'] = df['YEAR SOLD'].map(lambda x: int(x))\n",
    "df['MONTH SOLD'] = df['SALE DATE'].map(lambda x: x[5:7])\n",
    "df['MONTH SOLD'] = df['MONTH SOLD'].map(lambda x: int(x))\n",
    "df['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].map(lambda x: year_built(x))\n",
    "df['LAND SQUARE FEET'] = df['LAND SQUARE FEET'].map(lambda x: year_built(x))\n",
    "df['YEAR BUILT'][df['YEAR BUILT']==0]=np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    df.isnull(), \n",
    "    yticklabels=False, \n",
    "    cbar=False, \n",
    "    cmap='viridis'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='BOROUGH', data=df, palette='Set2')\n",
    "plt.title('Sales per Borough')\n",
    "\n",
    "# Maximum properties are sold in Queens! followed by Staten Island!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y='RESIDENTIAL UNITS', x='BOROUGH',data=df, palette='coolwarm', ci=None)\n",
    "plt.title('Sales per Borough Residential')\n",
    "\n",
    "# Residential Units are mainly sold in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y='COMMERCIAL UNITS', x='BOROUGH',data=df, palette='coolwarm', ci=None)\n",
    "plt.title('Sales per Borough Commercial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='YEAR SOLD', data=df, palette='rainbow')\n",
    "plt.title('Sales Rate from 2016-2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = df.groupby(['YEAR SOLD', 'BOROUGH'])['SALE PRICE'].mean().reset_index()\n",
    "\n",
    "# Barplot'u oluşturun\n",
    "sns.barplot(\n",
    "    x='YEAR SOLD', \n",
    "    y='SALE PRICE', \n",
    "    hue='BOROUGH', \n",
    "    data=summary_data, \n",
    "    palette='rainbow', \n",
    "    ci=None\n",
    ")\n",
    "plt.title('Sales per Borough from 2016-2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = df.groupby(['MONTH SOLD', 'BOROUGH'])['SALE PRICE'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(\n",
    "    x='MONTH SOLD', \n",
    "    y='SALE PRICE', \n",
    "    hue='BOROUGH', \n",
    "    data=summary_data, \n",
    "    palette='rainbow', \n",
    "    ci=None\n",
    ")\n",
    "plt.title('Sales per Borough from 2016-2017')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.countplot(x='MONTH SOLD', hue='YEAR SOLD', data=df, palette='RdBu_r')\n",
    "\n",
    "\"\"\"\n",
    "It is noticed that though the number of sales increased from the year 2016 to 2017, \n",
    "the sales prices per Borough(location) remained in the same ranges\n",
    "Also, the property prices are much higher at Manhattan than at any other location.\n",
    "As per months, property sales for 2017 took place from January till August, \n",
    "and for 2016 from September till December.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## DATA TYPE ANALYSIS ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING AGE'] = 2017 - df['YEAR BUILT']\n",
    "df['BOROUGH'] = df['BOROUGH'].map(cons.borough_encoding)\n",
    "df['SALE_YEAR'] = pd.DatetimeIndex(df[cons.date_column]).year\n",
    "df['SALE_MONTH'] = pd.DatetimeIndex(df[cons.date_column]).month\n",
    "\n",
    "\n",
    "del df[\"YEAR BUILT\"] \n",
    "del df[\"BUILDING CLASS AT PRESENT\"] # It contains the same information with BUILDING CLASS CATEGORY\n",
    "del df[\"ADDRESS\"] # The variable 1 not required for the model\n",
    "del df[\"ZIP CODE\"] # The variable 2 not required for the model\n",
    "del df[\"SALE DATE\"] # We don't need it as we have created the year and month variables.\n",
    "df.drop([\"EASE-MENT\",\"APARTMENT NUMBER\"], axis=1, inplace=True) # Completely empty columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df[cons.date_column] = pd.to_datetime(df[cons.date_column], errors='coerce') \n",
    "\n",
    "for col in cons.numeric_columns: \n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "for col in cons.categorical_columns: \n",
    "    df[col] = df[col].astype(object)\n",
    "\n",
    "def get_categorical_and_numeric_columns(dataframe, number_of_unique_classes):\n",
    "    categorical_columns = [col for col in dataframe.columns if len(dataframe[col].unique()) <= number_of_unique_classes\n",
    "                          or dataframe[col].dtype == \"O\"]\n",
    "    \n",
    "    new_categorical_columns = []\n",
    "    for cat_cols in categorical_columns:\n",
    "        new_categorical_columns.append(cat_cols)\n",
    "        \n",
    "    numeric_columns = [col for col in dataframe.columns if len(dataframe[col].unique()) > number_of_unique_classes\n",
    "                       and dataframe[col].dtype != \"O\" or col not in new_categorical_columns]\n",
    "    numeric_columns.remove(cons.target_variable)  \n",
    "    return new_categorical_columns, numeric_columns\n",
    "\n",
    "def unique_value_analysis(dataframe, number_of_unique_classes):\n",
    "    check_flag = 0\n",
    "    for i in range(0, len(number_of_unique_classes)):\n",
    "        cat_cols, num_cols = get_categorical_and_numeric_columns(dataframe, number_of_unique_classes[i])\n",
    "        print('Number of Unique Value:',number_of_unique_classes[i],' Number of Categorical Value:',len(cat_cols),' Number of Numerical Value: ', len(num_cols))\n",
    "        if check_flag < len(cat_cols)*1.1 and check_flag > 0:\n",
    "            return number_of_unique_classes[i]\n",
    "        else:\n",
    "            check_flag = len(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = unique_value_analysis(df,[2, 3, 5, 10, 20, 50])\n",
    "print(\"threshold_value: \", threshold_value)\n",
    "cat_cols, num_cols = get_categorical_and_numeric_columns(df, threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### FILLING MISSING VALUES #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MISSING DATA ANALYSIS\n",
    "\n",
    "df.replace(' ',np.nan, inplace=True)\n",
    "round(df.isna().sum() /len(df) *100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(dataframe):\n",
    "    variables_with_na = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "    n_miss = dataframe[variables_with_na].isnull().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe[variables_with_na].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "    missing_df.reset_index(inplace=True)\n",
    "    missing_df = missing_df.rename(columns = {'index':'Columns Name'})\n",
    "    return missing_df\n",
    "##################################################################################################################\n",
    "\n",
    "def missing_vs_target(dataframe,target,variable_with_na):\n",
    "    temp_df = dataframe.copy()\n",
    "    for variable in variable_with_na:\n",
    "        temp_df[variable + '_NA_FLAG'] = np.where(temp_df[variable].isnull(), 1, 0)    \n",
    "    flags_na = temp_df.loc[:, temp_df.columns.str.contains('_NA_')].columns\n",
    "    for variable in flags_na:\n",
    "        print(pd.DataFrame({'TARGET_MEAN': temp_df.groupby(variable)[target].mean()}),end='\\n\\n\\n')\n",
    "\n",
    "missing_df = missing_values_table(df)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_drop_list = missing_df[missing_df['ratio']>40]['Columns Name'].values.tolist()\n",
    "print(len(missing_drop_list), 'number of columns are removed.')\n",
    "df.drop(missing_drop_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting 0 price transfer transactions\n",
    "# Deleting nan values of dependence variable\n",
    "\n",
    "df = df[df[\"SALE PRICE\"] > 0]  \n",
    "df = df[df[\"SALE PRICE\"].notnull()] \n",
    "\n",
    "# Square feet should not be 0.\n",
    "df = df[df[\"LAND SQUARE FEET\"] != 0]\n",
    "df = df[df[\"GROSS SQUARE FEET\"] != 0]\n",
    "\n",
    "df = df.dropna() \n",
    "df.drop_duplicates(keep = \"last\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data where commercial + residential doesn't equal total units\n",
    "df = df[df['TOTAL UNITS'] == df['COMMERCIAL UNITS'] + df['RESIDENTIAL UNITS']]\n",
    "df[[\"TOTAL UNITS\", \"SALE PRICE\"]].groupby(['TOTAL UNITS'], as_index=False).count().sort_values(by='SALE PRICE', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with TOTAL UNITS == 0 and one outlier with 2261 units\n",
    "df = df[(df['TOTAL UNITS'] > 0) & (df['TOTAL UNITS'] != 2261)]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ OUTLIER ANALYSIS\n",
    "\n",
    "df_after_missing = df.copy()\n",
    "round(df.describe([0.75,0.85,0.95,0.99,0.995,0.999]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_thresholds(dataframe, variable):\n",
    "    quartile1 = dataframe[variable].quantile(0.25)\n",
    "    quartile3 = dataframe[variable].quantile(0.75)\n",
    "    interquantile_range = quartile3 - quartile1\n",
    "    up_limit = quartile3 + 1.5 * interquantile_range\n",
    "    low_limit = quartile1 - 1.5 * interquantile_range\n",
    "    return low_limit, up_limit\n",
    "##################################################################################################################\n",
    "\n",
    "def replace_with_thresholds(dataframe, columns_list):\n",
    "    for variable in columns_list:\n",
    "        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
    "        if low_limit < 0:\n",
    "            low_limit = 0\n",
    "        dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
    "        dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n",
    "##################################################################################################################\n",
    "\n",
    "def remove_outliers(dataframe, numeric_columns):\n",
    "    for variable in numeric_columns:\n",
    "        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
    "        dataframe_without_outliers = dataframe[~((dataframe[variable] < low_limit) | (dataframe[variable] > up_limit))]\n",
    "    return dataframe_without_outliers\n",
    "##################################################################################################################\n",
    "\n",
    "def has_outliers(dataframe, num_col_names, plot=False):\n",
    "    variable_names = []\n",
    "    for col in num_col_names:\n",
    "        low_limit, up_limit = outlier_thresholds(dataframe, col)\n",
    "        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n",
    "            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]\n",
    "            print(col, \":\", number_of_outliers)\n",
    "            variable_names.append(col)\n",
    "            if plot:\n",
    "                sns.boxplot(x=dataframe[col])\n",
    "                plt.show()\n",
    "    return variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_variable_names = has_outliers(df,num_cols)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_raw = df_raw.drop(\n",
    "    [\"NEIGHBORHOOD\",\"BUILDING CLASS CATEGORY\",\"TAX CLASS AT PRESENT\", \"BUILDING CLASS AT TIME OF SALE\"],\n",
    "    axis=1)\n",
    "\n",
    "# Correlations with target according to the progress of the study\n",
    "corr = pd.DataFrame(df.select_dtypes(include=['number']).corr().abs().unstack()\n",
    "                    .sort_values(ascending = False)[\"SALE PRICE\"][1:]).rename(columns = {0:\"Corr After Outlier and Missing\"})\n",
    "a = pd.DataFrame(df_after_missing.select_dtypes(include=['number']).corr().abs().unstack()\n",
    "                 .sort_values(ascending = False)[\"SALE PRICE\"][1:]).rename(columns = {0:\"Corr After Missing\"})\n",
    "b = pd.DataFrame(df_raw.corr().select_dtypes(include=['number']).abs().unstack()\n",
    "                 .sort_values(ascending = False)[\"SALE PRICE\"][1:]).rename(columns = {0:\"Corr Default\"}).iloc[:11,:]\n",
    "\n",
    "pd.concat([b,a,corr],axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"LOT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## VISUALIZE\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12,5))\n",
    "plotd = sns.distplot(df[(df['SALE PRICE']>100) & (df['SALE PRICE'] < 5000000)]['SALE PRICE'], kde=True, bins=100)\n",
    "\n",
    "tick_spacing=250000\n",
    "plotd.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "plotd.set_xlim([-100000, 5000000])\n",
    "plt.xticks(rotation=30)\n",
    "plt.axvline(df[(df['SALE PRICE']>100) & (df['SALE PRICE'] < 5000000)]['SALE PRICE'].mean(), c='red')\n",
    "plt.axvline(df[(df['SALE PRICE']>100) & (df['SALE PRICE'] < 5000000)]['SALE PRICE'].median(), c='blue')\n",
    "plt.text(250000,0.0000012, \"median\")\n",
    "plt.text(850000,0.0000010, \"mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df.select_dtypes(np.number)\n",
    "df[m.columns]= m.round().astype('Int64')\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "df.rename(columns = {\"SALE PRICE\":\"sale_price\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(df['sale_price'] > 100000) & (df['sale_price'] < 5000000)]\n",
    "\n",
    "trace0 = go.Box(\n",
    "    y=df2.sale_price[df2.BOROUGH == 'Manhattan' ],\n",
    "    name = 'Manhattan',\n",
    "    marker = dict(\n",
    "        color = 'rgb(12, 12, 140)',\n",
    "    )\n",
    ")\n",
    "trace1 = go.Box(\n",
    "    y=df2.sale_price[df2.BOROUGH ==  'Bronx' ],\n",
    "    name = 'Bronx',\n",
    "    marker = dict(\n",
    "        color = 'rgb(8,81,156)',\n",
    "    )\n",
    ")\n",
    "trace2 = go.Box(\n",
    "    y=df2.sale_price[df2.BOROUGH ==  'Brooklyn' ],\n",
    "    name = 'Brooklyn',\n",
    "    marker = dict(\n",
    "        color = 'rgb(12, 12, 140)',\n",
    "    )\n",
    ")\n",
    "trace3 = go.Box(\n",
    "    y=df2.sale_price[df2.BOROUGH ==  'Queens' ],\n",
    "    name = 'Queens',\n",
    "    marker = dict(\n",
    "        color = 'rgb(12, 128, 128)',\n",
    "    )\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    y=df2.sale_price[df2.BOROUGH ==  'Staten Island' ],\n",
    "    name = 'Staten Island',\n",
    "    marker = dict(\n",
    "        color = 'rgb(12, 12, 140)',\n",
    "    )\n",
    ")\n",
    "\n",
    "dat = [trace0, trace1, trace2, trace3, trace4]\n",
    "layout = go.Layout(\n",
    "    title='Housing Prices by Boroughs',\n",
    "    xaxis=dict(\n",
    "        title='Borough'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Sale Price'\n",
    "    ),\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=dat, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bar=df[['SALE_MONTH', 'sale_price']].groupby(by='SALE_MONTH').count().sort_values(by='SALE_MONTH', ascending=True).reset_index()\n",
    "df_bar.columns.values[1]='Sales_count'\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(y = 'Sales_count', x = 'SALE_MONTH', data = df_bar)\n",
    "plt.title('Sale Count by Months')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no seasonality for house prices. So we can delete theese columns.\n",
    "del df[\"SALE_YEAR\"]\n",
    "del df[\"SALE_MONTH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "order = sorted(df['BUILDING CLASS CATEGORY'].unique())\n",
    "sns.boxplot(x='BUILDING CLASS CATEGORY', y='sale_price', data=df, order=order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Sale Prices by Building Class Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[\n",
    "    (df['COMMERCIAL UNITS']<20) & (df['TOTAL UNITS']<50) & \n",
    "    (df['sale_price']<5000000) & (df['sale_price']>100000) & \n",
    "    (df['GROSS SQUARE FEET']>0)\n",
    "    ]\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(x='COMMERCIAL UNITS', y=\"sale_price\", data=dataset)\n",
    "plt.title('Sale Prices by Commercial Units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting of unnecessary categorical variables for the model\n",
    "\n",
    "del df[\"NEIGHBORHOOD\"]\n",
    "del df['BUILDING CLASS AT TIME OF SALE']\n",
    "del df['TAX CLASS AT PRESENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing all column names to be lowercase and no spaces\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "df.tax_class_at_time_of_sale = df.tax_class_at_time_of_sale.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\n",
    "    [\"sale_price\",\"building_class_category\",\"borough\",\"tax_class_at_time_of_sale\"],\n",
    "    axis = 1\n",
    "    ).astype(int)\n",
    "\n",
    "y = df[\"sale_price\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# BASE MODEL\n",
    "# Popular Regression Algorithms\n",
    "\n",
    "lm = LinearRegression()\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 42)\n",
    "\n",
    "\n",
    "algo = [lm, lgbm, rf]\n",
    "result = []\n",
    "\n",
    "for i in algo:\n",
    "        start = time.process_time()\n",
    "        model = i.fit(X_train,y_train)\n",
    "        result.append([str(i).split(\"(\")[0] + str(\"_baseline\"), model.score(X_train, y_train), model.score(X_test, y_test),\n",
    "                  np.sqrt(mean_squared_error(y_train, model.predict(X_train))),\n",
    "                  np.sqrt(mean_squared_error(y_test, model.predict(X_test))),\n",
    "                  mean_absolute_error(y_train, model.predict(X_train)),\n",
    "                  mean_absolute_error(y_test, model.predict(X_test))])\n",
    "        print(str(i).split(\"(\")[0],\"✓    \", \"{}\".format(time.process_time()-start),\"sn\")\n",
    "        \n",
    "result_raw_model = pd.DataFrame(result, columns = [\"Algorithm\", \"Train_Score\", \"Test_Score\", \"Train_Rmse\",\n",
    "                                         \"Test_Rmse\", \"Train_Mae\", \"Test_Mae\"]).sort_values(\"Test_Rmse\").set_index(\"Algorithm\")\n",
    "result_raw_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "# 1 - a) Multicolinarity Control ( High correlation of independent variables with each other )¶\n",
    "\n",
    "\n",
    "mtr = X.corr()\n",
    "mask = np.zeros_like(mtr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "plt.suptitle(\"Correlation Matrix of Columns in Heatmap\", size=24)\n",
    "sns.heatmap(mtr, mask= mask, annot=True, annot_kws={\"size\": 10});"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Identifying and deleting variables with more than 75% correlation\n",
    "\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "[column for column in upper.columns if any(upper[column] > 0.75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"residential_units\"]\n",
    "del df[\"total_units\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Identifying and deleting variables that have nothing to do with the dependent variable\n",
    "# 1 - b) Low Correlations with Dependent Variable\n",
    "\n",
    "df.corr().abs().unstack().sort_values(ascending =False )[\"sale_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"building_age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables from categorical variables\n",
    "\n",
    "one_hot_variables = [\"borough\",\"building_class_category\",\"tax_class_at_time_of_sale\"]\n",
    "df = pd.get_dummies(df,prefix = one_hot_variables, prefix_sep = \"_\" , drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"sale_price\"],axis = 1).astype(int)\n",
    "y = df[\"sale_price\"].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = [lgbm, lm, rf]\n",
    "result = []\n",
    "\n",
    "for i in algo:\n",
    "        start = time.process_time()\n",
    "        model = i.fit(X_train,y_train)\n",
    "        result.append([str(i).split(\"(\")[0] + str(\"_processed\"), model.score(X_train, y_train), model.score(X_test, y_test),\n",
    "                  np.sqrt(mean_squared_error(y_train, model.predict(X_train))),\n",
    "                  np.sqrt(mean_squared_error(y_test, model.predict(X_test))),\n",
    "                  mean_absolute_error(y_train, model.predict(X_train)),\n",
    "                  mean_absolute_error(y_test, model.predict(X_test))])\n",
    "        print(str(i).split(\"(\")[0],\"✓    \", \"{}\".format(time.process_time()-start),\"sec\")\n",
    "        \n",
    "result_second_model = pd.DataFrame(result, columns = [\"Algorithm\", \"Train_Score\", \"Test_Score\", \"Train_Rmse\",\n",
    "                                         \"Test_Rmse\", \"Train_Mae\", \"Test_Mae\"]).sort_values(\"Test_Rmse\").set_index(\"Algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_second_model.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_raw_model.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\":[100,200,300],\n",
    "    \"max_depth\":[10, 50, 100],\n",
    "    \"max_features\":[6,8,10,12,14,16]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "rf_tuned = GridSearchCV(estimator = rf,\n",
    "                            param_grid = param_grid,\n",
    "                            cv = 2,\n",
    "                            n_jobs=-1,\n",
    "                        verbose=0)\n",
    "\n",
    "rf_tuned.fit(X_train, y_train)\n",
    "rf_tuned.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf_tuned.best_estimator_.fit(X_train,y_train) # Grid Search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_optimize_model = []\n",
    "result_optimize_model.append([\"RandomForestRegressor Optimize\", model.score(X_train, y_train), model.score(X_test, y_test),\n",
    "                  np.sqrt(mean_squared_error(y_train, model.predict(X_train))),\n",
    "                  np.sqrt(mean_squared_error(y_test, model.predict(X_test))),\n",
    "                  mean_absolute_error(y_train, model.predict(X_train)),\n",
    "                  mean_absolute_error(y_test, model.predict(X_test))])\n",
    "\n",
    "pd.DataFrame(result_optimize_model, columns = [\"Algorithm\", \"Train_Score\", \"Test_Score\", \"Train_Rmse\",\n",
    "                                         \"Test_Rmse\", \"Train_Mae\", \"Test_Mae\"]).sort_values(\"Test_Rmse\").set_index(\"Algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_second_model[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = model.feature_importances_.tolist()\n",
    "importance = pd.DataFrame(sorted(zip(X_train.columns,rankings),reverse=True),columns=[\"variable\",\"importance\"]).sort_values(\"importance\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"variable\",\n",
    "            data=importance)\n",
    "plt.title('Değişken Önemleri')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".exp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
